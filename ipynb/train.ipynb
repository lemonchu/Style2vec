{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import FontSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 配置字体相关路径\n",
    "fonts_dir = \"./font_ds/fonts\"            # 字体文件夹路径\n",
    "text_file = \"./font_ds/cleaned_text.txt\" # 文本文件路径\n",
    "chars_file = \"./font_ds/chars.txt\"                  # 常用字文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading fonts and rendering characters:   0%|          | 0/512 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading fonts and rendering characters: 100%|██████████| 512/512 [21:07<00:00,  2.48s/it]\n",
      "Loading fonts and rendering characters: 100%|██████████| 56/56 [02:05<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# 初始化 FontSampler，同时会将字体分为 train/test 两类\n",
    "sampler = FontSampler(fonts_dir, text_file, chars_file, font_size=76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------------------------\n",
    "# 定义模型并迁移到设备上，输出嵌入向量\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT).to(device)\n",
    "embedding_dim = 128  # 输出嵌入向量的维度\n",
    "hidden_dim = 256     # 隐藏层维度\n",
    "\n",
    "# 修改最后全连接层，直接输出嵌入向量\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, hidden_dim),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(hidden_dim, hidden_dim),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(hidden_dim, embedding_dim)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_loss_and_acc(style_vecs, group_size):\n",
    "    \"\"\"\n",
    "    计算交叉熵损失和准确率。\n",
    "    对于每一行，重新生成一个 tensor，直接去除对角线（即自身的分数）。\n",
    "\n",
    "    :param style_vecs: 向量序列，由模型生成，形状为 [N, embedding_dim]\n",
    "    :param group_size: 每组的大小\n",
    "    :return: loss 和 acc\n",
    "    \"\"\"\n",
    "    similarity_matrix = torch.matmul(style_vecs, style_vecs.T)  # shape: [N, N]\n",
    "    N = similarity_matrix.size(0)\n",
    "    losses = []\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        row = similarity_matrix[i]  # shape: [N]\n",
    "        # 重新构造一个 tensor，去除自身的分数（第 i 个元素）\n",
    "        new_row = torch.cat((row[:i], row[i+1:]))  # shape: [N-1]\n",
    "        \n",
    "        # 对新 row 计算 log_softmax\n",
    "        log_softmax_row = F.log_softmax(new_row, dim=0)\n",
    "        \n",
    "        # 构造目标分布：对于当前行所属的组（group_start 到 group_end-1），除去自身，每个目标均为 1/(group_size-1)\n",
    "        target = torch.zeros_like(new_row)\n",
    "        group_start = (i // group_size) * group_size\n",
    "        group_end = group_start + group_size -1\n",
    "        target[group_start:group_end] = 1.0 / (group_size - 1)\n",
    "        \n",
    "        # 计算 KL 散度损失\n",
    "        row_loss = F.kl_div(log_softmax_row, target, reduction='sum')\n",
    "        losses.append(row_loss)\n",
    "\n",
    "        # 计算准确率：\n",
    "        # 从 new_row 选取 top-(group_size-1)，如果这些位置对应的原始索引均落在同一组中，则算作正确\n",
    "        topk_indices = new_row.topk(group_size - 1).indices\n",
    "        correct_in_row = ((topk_indices >= group_start) & (topk_indices < group_end)).sum().item()\n",
    "        correct += correct_in_row\n",
    "\n",
    "    loss = torch.stack(losses).mean()\n",
    "    acc = correct / ((group_size - 1) * N)\n",
    "\n",
    "    # 以 1e-3 的概率展示相似度矩阵 softmax\n",
    "    if random.random() < 1e-3:\n",
    "        softmax_matrix = F.softmax(similarity_matrix, dim=1)\n",
    "        print(f\"Softmax Matrix: {softmax_matrix}\")\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 训练和验证步骤（loss 基于 word2vec 风格的 compute_loss）\n",
    "def train_step(model, epoch, data_loader, optimizer, batch_size, font_size, group_size):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    progress_bar = tqdm(data_loader, desc=f'Epoch {epoch + 1} - Training', leave=True)\n",
    "    for batch in progress_bar:\n",
    "        # Flatten the batch into a single tensor\n",
    "        flattened_batch = [img.to(device) for sample in batch for img in sample]  # Flatten the nested list\n",
    "        batch_tensor = torch.stack(flattened_batch).squeeze(1)  # Shape: [total_images_in_batch, C, H, W]\n",
    "\n",
    "        # Pass the entire batch through the model\n",
    "        style_vecs = model(batch_tensor)  # Shape: [total_images_in_batch, embedding_dim]\n",
    "\n",
    "        # Reshape the output to match the expected input shape for compute_loss_and_acc\n",
    "        style_vecs = style_vecs.view(batch_size, font_size * group_size, -1)  # Shape: [batch_size, group_size, embedding_dim]\n",
    "        \n",
    "        # Compute the loss and accuracy\n",
    "        loss, acc = 0, 0\n",
    "        for i in range(batch_size):\n",
    "            sample_loss, sample_acc = compute_loss_and_acc(style_vecs[i], group_size)\n",
    "            loss += sample_loss\n",
    "            acc += sample_acc\n",
    "\n",
    "        loss /= batch_size\n",
    "        acc /= batch_size\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        progress_bar.set_postfix(loss=loss.item(), acc=acc)\n",
    "    progress_bar.close()\n",
    "\n",
    "    return total_loss / len(data_loader), total_acc / len(data_loader)\n",
    "\n",
    "def validate(model, data_loader, batch_size, font_size, group_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(data_loader, desc=\"Validating\", leave=True)\n",
    "        for batch in progress_bar:\n",
    "            # Flatten the batch into a single tensor\n",
    "            flattened_batch = [img.to(device) for sample in batch for img in sample]  # Flatten the nested list\n",
    "            batch_tensor = torch.stack(flattened_batch).squeeze(1)  # Shape: [total_images_in_batch, C, H, W]\n",
    "\n",
    "            # Pass the entire batch through the model\n",
    "            style_vecs = model(batch_tensor)  # Shape: [total_images_in_batch, embedding_dim]\n",
    "\n",
    "            # Reshape the output to match the expected input shape for compute_loss_and_acc\n",
    "            style_vecs = style_vecs.view(batch_size, font_size * group_size, -1) # Shape: [batch_size, group_size, embedding_dim]\n",
    "            \n",
    "            # Compute the loss and accuracy\n",
    "            loss, acc = 0, 0\n",
    "            for i in range(batch_size):\n",
    "                sample_loss, sample_acc = compute_loss_and_acc(style_vecs[i], group_size)\n",
    "                loss += sample_loss\n",
    "                acc += sample_acc\n",
    "\n",
    "            loss /= batch_size\n",
    "            acc /= batch_size\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            progress_bar.set_postfix(loss=loss.item(), acc=acc)\n",
    "        progress_bar.close()\n",
    "\n",
    "    return total_loss / len(data_loader), total_acc / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations for the image 数据\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转为张量\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道图像复制为3通道\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet 标准化\n",
    "])\n",
    "\n",
    "# 定义一个简单的 Dataset 类来处理样本\n",
    "class FontDataset(Dataset):\n",
    "    def __init__(self, batchs, transform=None):\n",
    "        self.batchs = batchs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batchs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.batchs[idx]\n",
    "        if self.transform:\n",
    "            batch = [[self.transform(img) for img in inner_list] for inner_list in batch]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器（只包含 model 参数）\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=4e-4, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# 迭代次数，可根据需求调整\n",
    "num_epochs = 16\n",
    "epoch_length = 64  # 每个 epoch 中的 batch 个数\n",
    "\n",
    "# 假设每次采样返回的样本中，同一字体的样本数等于 sample_cnt，此处作为 group_size\n",
    "font_cnt = 4\n",
    "sample_cnt = 4\n",
    "batch_size = 16  # 每个批次的样本数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Collecting val samples:   0%|          | 0/128 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Collecting val samples: 100%|██████████| 128/128 [00:11<00:00, 11.32it/s]\n",
      "Epoch 1 - Collecting train samples: 100%|██████████| 1024/1024 [01:31<00:00, 11.24it/s]\n",
      "Epoch 1 - Training:   8%|▊         | 5/64 [00:55<11:04, 11.27s/it, acc=0.62, loss=1.01] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Matrix: tensor([[2.1760e-01, 4.9415e-02, 1.0879e-01, 7.5385e-02, 1.1328e-01, 7.6593e-02,\n",
      "         9.3581e-02, 5.0821e-02, 3.1456e-02, 4.4698e-02, 1.5908e-02, 3.1293e-02,\n",
      "         3.2005e-02, 9.1795e-03, 2.2951e-02, 2.7042e-02],\n",
      "        [5.2187e-02, 1.5802e-01, 4.3337e-02, 6.6426e-02, 3.9180e-02, 1.6906e-02,\n",
      "         1.9005e-02, 2.4392e-02, 6.6994e-02, 5.5922e-02, 7.0691e-02, 7.5998e-02,\n",
      "         7.2869e-02, 9.5699e-02, 4.6463e-02, 9.5905e-02],\n",
      "        [7.4003e-02, 2.7915e-02, 2.9289e-01, 8.0253e-02, 1.1095e-01, 6.2529e-02,\n",
      "         2.3361e-01, 5.6900e-02, 1.0613e-02, 1.7674e-02, 5.1722e-03, 7.5090e-03,\n",
      "         6.4663e-03, 2.5774e-03, 5.5598e-03, 5.3755e-03],\n",
      "        [7.3390e-02, 6.1234e-02, 1.1485e-01, 2.9552e-01, 7.0310e-02, 8.6691e-02,\n",
      "         5.2311e-02, 7.5625e-02, 2.8299e-02, 1.6234e-02, 3.0044e-02, 2.3507e-02,\n",
      "         1.1348e-02, 1.3855e-02, 2.9551e-02, 1.7231e-02],\n",
      "        [9.8991e-02, 3.2418e-02, 1.4252e-01, 6.3109e-02, 1.9625e-01, 9.0128e-02,\n",
      "         2.1312e-01, 7.6126e-02, 1.2125e-02, 2.1902e-02, 6.2661e-03, 1.1150e-02,\n",
      "         1.2959e-02, 3.7352e-03, 9.3125e-03, 9.8875e-03],\n",
      "        [6.6764e-02, 1.3954e-02, 8.0123e-02, 7.7621e-02, 8.9907e-02, 3.7499e-01,\n",
      "         1.2130e-01, 1.1375e-01, 1.0080e-02, 8.9824e-03, 6.7468e-03, 8.1854e-03,\n",
      "         5.6181e-03, 2.8918e-03, 1.2969e-02, 6.1142e-03],\n",
      "        [4.2056e-02, 8.0876e-03, 1.5434e-01, 2.4148e-02, 1.0961e-01, 6.2541e-02,\n",
      "         5.3022e-01, 4.5226e-02, 3.5246e-03, 9.8026e-03, 1.2852e-03, 2.5473e-03,\n",
      "         2.5871e-03, 5.8359e-04, 1.8086e-03, 1.6362e-03],\n",
      "        [6.7982e-02, 3.0896e-02, 1.1189e-01, 1.0391e-01, 1.1654e-01, 1.7456e-01,\n",
      "         1.3462e-01, 1.4609e-01, 1.7583e-02, 1.7641e-02, 1.3075e-02, 1.4996e-02,\n",
      "         1.2768e-02, 7.2996e-03, 1.7724e-02, 1.2433e-02],\n",
      "        [3.3017e-02, 6.6583e-02, 1.6375e-02, 3.0511e-02, 1.4564e-02, 1.2137e-02,\n",
      "         8.2317e-03, 1.3796e-02, 1.3844e-01, 6.6300e-02, 1.3115e-01, 1.2207e-01,\n",
      "         6.0056e-02, 1.3808e-01, 5.6387e-02, 9.2300e-02],\n",
      "        [6.5395e-02, 7.7469e-02, 3.8011e-02, 2.4397e-02, 3.6669e-02, 1.5076e-02,\n",
      "         3.1912e-02, 1.9294e-02, 9.2414e-02, 1.3973e-01, 5.7791e-02, 9.5139e-02,\n",
      "         1.0364e-01, 7.6919e-02, 3.8295e-02, 8.7852e-02],\n",
      "        [1.0524e-02, 4.4280e-02, 5.0298e-03, 2.0415e-02, 4.7438e-03, 5.1202e-03,\n",
      "         1.8919e-03, 6.4661e-03, 8.2661e-02, 2.6131e-02, 1.9605e-01, 9.0408e-02,\n",
      "         5.4605e-02, 2.7348e-01, 7.2153e-02, 1.0604e-01],\n",
      "        [2.6238e-02, 6.0335e-02, 9.2550e-03, 2.0245e-02, 1.0699e-02, 7.8732e-03,\n",
      "         4.7524e-03, 9.3992e-03, 9.7510e-02, 5.4523e-02, 1.1458e-01, 1.2926e-01,\n",
      "         9.0413e-02, 1.7001e-01, 6.4373e-02, 1.3053e-01],\n",
      "        [1.5529e-02, 3.3478e-02, 4.6120e-03, 5.6559e-03, 7.1958e-03, 3.1272e-03,\n",
      "         2.7931e-03, 4.6311e-03, 2.7762e-02, 3.4371e-02, 4.0050e-02, 5.2322e-02,\n",
      "         3.2170e-01, 2.0649e-01, 5.0529e-02, 1.8975e-01],\n",
      "        [1.5567e-03, 1.5367e-02, 6.4251e-04, 2.4134e-03, 7.2488e-04, 5.6258e-04,\n",
      "         2.2021e-04, 9.2540e-04, 2.2309e-02, 8.9158e-03, 7.0106e-02, 3.4385e-02,\n",
      "         7.2172e-02, 6.1750e-01, 4.3150e-02, 1.0905e-01],\n",
      "        [2.0978e-02, 4.0213e-02, 7.4703e-03, 2.7745e-02, 9.7409e-03, 1.3599e-02,\n",
      "         3.6784e-03, 1.2111e-02, 4.9104e-02, 2.3925e-02, 9.9692e-02, 7.0177e-02,\n",
      "         9.5187e-02, 2.3257e-01, 1.5664e-01, 1.3717e-01],\n",
      "        [1.1600e-02, 3.8955e-02, 3.3897e-03, 7.5925e-03, 4.8538e-03, 3.0089e-03,\n",
      "         1.5618e-03, 3.9871e-03, 3.7722e-02, 2.5759e-02, 6.8761e-02, 6.6783e-02,\n",
      "         1.6776e-01, 2.7586e-01, 6.4376e-02, 2.1803e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training:  30%|██▉       | 19/64 [03:40<08:41, 11.60s/it, acc=0.803, loss=0.578]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m FontDataset(val_batches, transform\u001b[38;5;241m=\u001b[39mdata_transforms)\n\u001b[1;32m     42\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 44\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_cnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_cnt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, val_loader, batch_size, font_cnt, sample_cnt)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, epoch, data_loader, optimizer, batch_size, font_size, group_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sample(sampler, font_cnt, sample_cnt, sample_source):\n",
    "    sample = sampler.sample(font_cnt=font_cnt, sample_cnt=sample_cnt, sample_source=sample_source)\n",
    "    return sample\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 收集一个 epoch 所需的所有训练样本\n",
    "    train_samples = []\n",
    "    val_samples = []\n",
    "\n",
    "    # 使用多线程采样所有数据\n",
    "    total_samples = epoch_length * batch_size\n",
    "    val_samples_count = total_samples // 8  # 1/8 的数据用于验证\n",
    "    train_samples_count = total_samples\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        val_futures = [executor.submit(sample, sampler, font_cnt, sample_cnt, \"test\") for _ in range(val_samples_count)]\n",
    "        val_samples = [future.result() for future in tqdm(val_futures, desc=f\"Epoch {epoch + 1} - Collecting val samples\")]\n",
    "\n",
    "        train_futures = [executor.submit(sample, sampler, font_cnt, sample_cnt, \"train\") for _ in range(train_samples_count)]\n",
    "        train_samples = [future.result() for future in tqdm(train_futures, desc=f\"Epoch {epoch + 1} - Collecting train samples\")]\n",
    "\n",
    "    # 将采样结果重新排布为 [epoch_length, batch_size] 的格式\n",
    "    train_batches = []\n",
    "    for i in range(epoch_length):\n",
    "        batch_samples = train_samples[i * batch_size:(i + 1) * batch_size]\n",
    "        train_batches.append(batch_samples)\n",
    "\n",
    "    val_batches = []\n",
    "    val_length = len(val_samples) // batch_size\n",
    "    for i in range(val_length):\n",
    "        batch_samples = val_samples[i * batch_size:(i + 1) * batch_size]\n",
    "        val_batches.append(batch_samples)\n",
    "\n",
    "    # 创建训练集 Dataset 和 DataLoader\n",
    "    train_dataset = FontDataset(train_batches, transform=data_transforms)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # 创建验证集 Dataset 和 DataLoader\n",
    "    val_dataset = FontDataset(val_batches, transform=data_transforms)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    train_loss, train_acc = train_step(model, epoch, train_loader, optimizer, batch_size, font_cnt, sample_cnt)\n",
    "    val_loss, val_acc = validate(model, val_loader, batch_size, font_cnt, sample_cnt)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # 保存模型\n",
    "    model_save_path = f'font_style2vec_model_epoch_{epoch + 1}.pth'\n",
    "    torch.save(model, model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
