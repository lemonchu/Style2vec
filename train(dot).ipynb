{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import FontSampler\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 配置字体相关路径\n",
    "fonts_dir = \"./font_ds/fonts\"            # 字体文件夹路径\n",
    "text_file = \"./font_ds/cleaned_text.txt\" # 文本文件路径\n",
    "chars_file = \"./font_ds/chars.txt\"                  # 常用字文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading fonts and rendering characters: 100%|██████████| 28/28 [00:49<00:00,  1.75s/it]\n",
      "Loading fonts and rendering characters: 100%|██████████| 28/28 [00:51<00:00,  1.83s/it]\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# 初始化 FontSampler，同时会将字体分为 train/test 两类\n",
    "sampler = FontSampler(fonts_dir, text_file, chars_file, font_size=76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------------------------\n",
    "# 定义模型并迁移到设备上，输出嵌入向量\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT).to(device)\n",
    "embedding_dim = 128  # 输出嵌入向量的维度\n",
    "hidden_dim = 256     # 隐藏层维度\n",
    "\n",
    "# 修改最后全连接层，直接输出嵌入向量\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, hidden_dim),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(hidden_dim, hidden_dim),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(hidden_dim, embedding_dim)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_loss_and_acc(style_vecs, group_size):\n",
    "    \"\"\"\n",
    "    计算交叉熵损失和准确率。\n",
    "    对于每一行，重新生成一个 tensor，直接去除对角线（即自身的分数）。\n",
    "\n",
    "    :param style_vecs: 向量序列，由模型生成，形状为 [N, embedding_dim]\n",
    "    :param group_size: 每组的大小\n",
    "    :return: loss 和 acc\n",
    "    \"\"\"\n",
    "\n",
    "    # 对 style_vecs 进行 L2 标准化\n",
    "    style_vecs = F.normalize(style_vecs, p=2, dim=1)\n",
    "\n",
    "    # print(f\"Style Vecs: {style_vecs}\")\n",
    "    \n",
    "    # 计算点积，然后对 0 取 max，再提升到 alpha 次方\n",
    "    alpha = 4.0\n",
    "    dot_prod = torch.matmul(style_vecs, style_vecs.T)\n",
    "\n",
    "    # print(f\"Similarity Matrix: {dot_prod}\")\n",
    "\n",
    "\n",
    "    similarity_matrix = torch.clamp(dot_prod, min=1e-8) ** alpha\n",
    "    # print(f\"Similarity Matrix: {similarity_matrix}\")\n",
    "\n",
    "    # time.sleep(1)\n",
    "\n",
    "    N = similarity_matrix.size(0)\n",
    "    losses = []\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        row = similarity_matrix[i]  # shape: [N]\n",
    "        # 重新构造一个 tensor，去除自身的分数（第 i 个元素）\n",
    "        new_row = torch.cat((row[:i], row[i+1:]))  # shape: [N-1]\n",
    "        # 对 new_row 进行归一化\n",
    "        new_row = F.normalize(new_row, p=1, dim=0)\n",
    "        \n",
    "        # 构造目标分布：对于当前行所属的组（group_start 到 group_end-1），除去自身，每个目标均为 1/(group_size-1)\n",
    "        target = torch.zeros_like(new_row)\n",
    "        group_start = (i // group_size) * group_size\n",
    "        group_end = group_start + group_size -1\n",
    "        target[group_start:group_end] = 1.0 / (group_size - 1)\n",
    "        \n",
    "        # 计算 KL 散度损失\n",
    "        row_loss = F.kl_div(new_row.log(), target, reduction='sum')\n",
    "        losses.append(row_loss)\n",
    "\n",
    "        # 计算准确率：\n",
    "        # 从 new_row 选取 top-(group_size-1)，如果这些位置对应的原始索引均落在同一组中，则算作正确\n",
    "        topk_indices = new_row.topk(group_size - 1).indices\n",
    "        correct_in_row = ((topk_indices >= group_start) & (topk_indices < group_end)).sum().item()\n",
    "        correct += correct_in_row\n",
    "\n",
    "    loss = torch.stack(losses).mean()\n",
    "    acc = correct / ((group_size - 1) * N)\n",
    "\n",
    "    # 以 1e-3 的概率展示相似度矩阵 softmax\n",
    "    if random.random() < 1e-3:\n",
    "        print(f\"Sim Matrix: {similarity_matrix}\")\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 训练和验证步骤（loss 基于 word2vec 风格的 compute_loss）\n",
    "def train_step(model, epoch, data_loader, optimizer, batch_size, font_size, group_size):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    progress_bar = tqdm(data_loader, desc=f'Epoch {epoch + 1} - Training', leave=True)\n",
    "    for batch in progress_bar:\n",
    "        # Flatten the batch into a single tensor\n",
    "        flattened_batch = [img.to(device) for sample in batch for img in sample]  # Flatten the nested list\n",
    "        batch_tensor = torch.stack(flattened_batch).squeeze(1)  # Shape: [total_images_in_batch, C, H, W]\n",
    "\n",
    "        # Pass the entire batch through the model\n",
    "        style_vecs = model(batch_tensor)  # Shape: [total_images_in_batch, embedding_dim]\n",
    "\n",
    "        # Reshape the output to match the expected input shape for compute_loss_and_acc\n",
    "        style_vecs = style_vecs.view(batch_size, font_size * group_size, -1)  # Shape: [batch_size, group_size, embedding_dim]\n",
    "        \n",
    "        # Compute the loss and accuracy\n",
    "        loss, acc = 0, 0\n",
    "        for i in range(batch_size):\n",
    "            sample_loss, sample_acc = compute_loss_and_acc(style_vecs[i], group_size)\n",
    "            loss += sample_loss\n",
    "            acc += sample_acc\n",
    "\n",
    "        loss /= batch_size\n",
    "        acc /= batch_size\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        progress_bar.set_postfix(loss=loss.item(), acc=acc)\n",
    "    progress_bar.close()\n",
    "\n",
    "    return total_loss / len(data_loader), total_acc / len(data_loader)\n",
    "\n",
    "def validate(model, data_loader, batch_size, font_size, group_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(data_loader, desc=\"Validating\", leave=True)\n",
    "        for batch in progress_bar:\n",
    "            # Flatten the batch into a single tensor\n",
    "            flattened_batch = [img.to(device) for sample in batch for img in sample]  # Flatten the nested list\n",
    "            batch_tensor = torch.stack(flattened_batch).squeeze(1)  # Shape: [total_images_in_batch, C, H, W]\n",
    "\n",
    "            # Pass the entire batch through the model\n",
    "            style_vecs = model(batch_tensor)  # Shape: [total_images_in_batch, embedding_dim]\n",
    "\n",
    "            # Reshape the output to match the expected input shape for compute_loss_and_acc\n",
    "            style_vecs = style_vecs.view(batch_size, font_size * group_size, -1) # Shape: [batch_size, group_size, embedding_dim]\n",
    "            \n",
    "            # Compute the loss and accuracy\n",
    "            loss, acc = 0, 0\n",
    "            for i in range(batch_size):\n",
    "                sample_loss, sample_acc = compute_loss_and_acc(style_vecs[i], group_size)\n",
    "                loss += sample_loss\n",
    "                acc += sample_acc\n",
    "\n",
    "            loss /= batch_size\n",
    "            acc /= batch_size\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            progress_bar.set_postfix(loss=loss.item(), acc=acc)\n",
    "        progress_bar.close()\n",
    "\n",
    "    return total_loss / len(data_loader), total_acc / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations for the image 数据\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转为张量\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道图像复制为3通道\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet 标准化\n",
    "])\n",
    "\n",
    "# 定义一个简单的 Dataset 类来处理样本\n",
    "class FontDataset(Dataset):\n",
    "    def __init__(self, batchs, transform=None):\n",
    "        self.batchs = batchs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batchs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.batchs[idx]\n",
    "        if self.transform:\n",
    "            batch = [[self.transform(img) for img in inner_list] for inner_list in batch]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器（只包含 model 参数）\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=4e-4, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# 迭代次数，可根据需求调整\n",
    "num_epochs = 16\n",
    "epoch_length = 16  # 每个 epoch 中的 batch 个数\n",
    "\n",
    "# 假设每次采样返回的样本中，同一字体的样本数等于 sample_cnt，此处作为 group_size\n",
    "font_cnt = 2\n",
    "sample_cnt = 2\n",
    "batch_size = 8  # 每个批次的样本数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Collecting val samples:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Collecting val samples: 100%|██████████| 16/16 [00:00<00:00, 76.14it/s]\n",
      "Epoch 1 - Collecting train samples: 100%|██████████| 128/128 [00:01<00:00, 71.94it/s]\n",
      "Epoch 1 - Training:  44%|████▍     | 7/16 [00:00<00:01,  7.03it/s, acc=0.781, loss=7.88] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim Matrix: tensor([[1.0000e+00, 6.4500e-01, 1.0000e-32, 1.0000e-32],\n",
      "        [6.4500e-01, 1.0000e+00, 1.0000e-32, 1.0000e-32],\n",
      "        [1.0000e-32, 1.0000e-32, 1.0000e+00, 8.1512e-01],\n",
      "        [1.0000e-32, 1.0000e-32, 8.1512e-01, 1.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<PowBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|██████████| 16/16 [00:02<00:00,  6.84it/s, acc=0.75, loss=0.488] \n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00,  7.05it/s, acc=0.562, loss=0.979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16 - Train Loss: 1.1565, Train Acc: 0.8457, Val Loss: 0.8873, Val Acc: 0.6250\n",
      "Model saved to font_identifier_model(dot)_epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Collecting val samples: 100%|██████████| 16/16 [00:00<00:00, 75.00it/s]\n",
      "Epoch 2 - Collecting train samples: 100%|██████████| 128/128 [00:01<00:00, 71.20it/s]\n",
      "Epoch 2 - Training: 100%|██████████| 16/16 [00:02<00:00,  6.67it/s, acc=0.969, loss=0.368]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00,  7.26it/s, acc=0.781, loss=0.693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/16 - Train Loss: 0.4110, Train Acc: 0.8965, Val Loss: 0.6148, Val Acc: 0.8750\n",
      "Model saved to font_identifier_model(dot)_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Collecting val samples: 100%|██████████| 16/16 [00:00<00:00, 74.73it/s]\n",
      "Epoch 3 - Collecting train samples: 100%|██████████| 128/128 [00:01<00:00, 67.88it/s]\n",
      "Epoch 3 - Training: 100%|██████████| 16/16 [00:02<00:00,  7.06it/s, acc=0.906, loss=0.15] \n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00,  5.42it/s, acc=0.969, loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/16 - Train Loss: 0.2227, Train Acc: 0.9258, Val Loss: 0.4722, Val Acc: 0.7656\n",
      "Model saved to font_identifier_model(dot)_epoch_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Collecting val samples: 100%|██████████| 16/16 [00:00<00:00, 74.43it/s]\n",
      "Epoch 4 - Collecting train samples: 100%|██████████| 128/128 [00:01<00:00, 70.93it/s]\n",
      "Epoch 4 - Training: 100%|██████████| 16/16 [00:02<00:00,  7.46it/s, acc=1, loss=0.177]    \n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00,  5.34it/s, acc=1, loss=0.00345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/16 - Train Loss: 0.2498, Train Acc: 0.9395, Val Loss: 0.0610, Val Acc: 1.0000\n",
      "Model saved to font_identifier_model(dot)_epoch_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Collecting val samples: 100%|██████████| 16/16 [00:00<00:00, 76.82it/s]\n",
      "Epoch 5 - Collecting train samples: 100%|██████████| 128/128 [00:01<00:00, 72.72it/s]\n",
      "Epoch 5 - Training: 100%|██████████| 16/16 [00:02<00:00,  7.44it/s, acc=0.844, loss=0.447]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00,  7.25it/s, acc=0.906, loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/16 - Train Loss: 0.2626, Train Acc: 0.9375, Val Loss: 2.5437, Val Acc: 0.8594\n",
      "Model saved to font_identifier_model(dot)_epoch_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Collecting val samples: 100%|██████████| 16/16 [00:00<00:00, 81.20it/s]\n",
      "Epoch 6 - Collecting train samples: 100%|██████████| 128/128 [00:01<00:00, 72.42it/s]\n",
      "Epoch 6 - Training: 100%|██████████| 16/16 [00:02<00:00,  6.62it/s, acc=0.906, loss=0.327]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00,  6.88it/s, acc=0.812, loss=0.442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/16 - Train Loss: 0.3404, Train Acc: 0.9199, Val Loss: 0.4238, Val Acc: 0.7969\n",
      "Model saved to font_identifier_model(dot)_epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Collecting val samples: 100%|██████████| 16/16 [00:00<00:00, 77.49it/s]\n",
      "Epoch 7 - Collecting train samples: 100%|██████████| 128/128 [00:01<00:00, 73.67it/s]\n",
      "Epoch 7 - Training: 100%|██████████| 16/16 [00:02<00:00,  6.79it/s, acc=0.969, loss=0.107]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00,  6.98it/s, acc=0.844, loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/16 - Train Loss: 0.1756, Train Acc: 0.9375, Val Loss: 0.2832, Val Acc: 0.8594\n",
      "Model saved to font_identifier_model(dot)_epoch_7.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Collecting val samples: 100%|██████████| 16/16 [00:00<00:00, 76.56it/s]\n",
      "Epoch 8 - Collecting train samples: 100%|██████████| 128/128 [00:01<00:00, 70.52it/s]\n",
      "Epoch 8 - Training: 100%|██████████| 16/16 [00:02<00:00,  6.68it/s, acc=0.75, loss=4.24]  \n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00,  7.18it/s, acc=0.781, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/16 - Train Loss: 2.0353, Train Acc: 0.8770, Val Loss: 0.9899, Val Acc: 0.7656\n",
      "Model saved to font_identifier_model(dot)_epoch_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Collecting val samples: 100%|██████████| 16/16 [00:00<00:00, 76.53it/s]\n",
      "Epoch 9 - Collecting train samples: 100%|██████████| 128/128 [00:01<00:00, 72.94it/s]\n",
      "Epoch 9 - Training:  38%|███▊      | 6/16 [00:00<00:01,  6.58it/s, acc=0.938, loss=0.557]"
     ]
    }
   ],
   "source": [
    "def sample(sampler, font_cnt, sample_cnt, sample_source):\n",
    "    sample = sampler.sample(font_cnt=font_cnt, sample_cnt=sample_cnt, sample_source=sample_source)\n",
    "    return sample\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 收集一个 epoch 所需的所有训练样本\n",
    "    train_samples = []\n",
    "    val_samples = []\n",
    "\n",
    "    # 使用多线程采样所有数据\n",
    "    total_samples = epoch_length * batch_size\n",
    "    val_samples_count = total_samples // 8  # 1/8 的数据用于验证\n",
    "    train_samples_count = total_samples\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        val_futures = [executor.submit(sample, sampler, font_cnt, sample_cnt, \"test\") for _ in range(val_samples_count)]\n",
    "        val_samples = [future.result() for future in tqdm(val_futures, desc=f\"Epoch {epoch + 1} - Collecting val samples\")]\n",
    "\n",
    "        train_futures = [executor.submit(sample, sampler, font_cnt, sample_cnt, \"train\") for _ in range(train_samples_count)]\n",
    "        train_samples = [future.result() for future in tqdm(train_futures, desc=f\"Epoch {epoch + 1} - Collecting train samples\")]\n",
    "\n",
    "    # 将采样结果重新排布为 [epoch_length, batch_size] 的格式\n",
    "    train_batches = []\n",
    "    for i in range(epoch_length):\n",
    "        batch_samples = train_samples[i * batch_size:(i + 1) * batch_size]\n",
    "        train_batches.append(batch_samples)\n",
    "\n",
    "    val_batches = []\n",
    "    val_length = len(val_samples) // batch_size\n",
    "    for i in range(val_length):\n",
    "        batch_samples = val_samples[i * batch_size:(i + 1) * batch_size]\n",
    "        val_batches.append(batch_samples)\n",
    "\n",
    "    # 创建训练集 Dataset 和 DataLoader\n",
    "    train_dataset = FontDataset(train_batches, transform=data_transforms)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # 创建验证集 Dataset 和 DataLoader\n",
    "    val_dataset = FontDataset(val_batches, transform=data_transforms)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    train_loss, train_acc = train_step(model, epoch, train_loader, optimizer, batch_size, font_cnt, sample_cnt)\n",
    "    val_loss, val_acc = validate(model, val_loader, batch_size, font_cnt, sample_cnt)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # 保存模型\n",
    "    model_save_path = f'font_identifier_model(dot)_epoch_{epoch + 1}.pth'\n",
    "    torch.save(model, model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
